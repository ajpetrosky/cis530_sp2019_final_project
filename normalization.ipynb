{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "normalization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vxfpDNOvMLEa",
        "colab_type": "code",
        "outputId": "e891461a-3306-416d-84bb-42e0a14a15d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L3r6-1lMOyVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/'My Drive'/'Term Project 530'/Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8OAKBS_pMPUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up directory variables\n",
        "data_dir = \"/content/gdrive/My Drive/Term Project 530/Data/\"\n",
        "\n",
        "# student_genie_file = data_dir + \"StudentGenieMessages.csv\"\n",
        "# student_genie_teacher_file = data_dir + \"StudentTeacherMessages.csv\"\n",
        "\n",
        "# short_student_genie_file = data_dir + \"short_StudentGenie.csv\"\n",
        "\n",
        "first_input = data_dir + \"first_5000_StudentGenie.csv\"\n",
        "second_input = data_dir + \"second_5000_StudentGenie.csv\"\n",
        "third_input = data_dir + \"third_5000_StudentGenie.csv\"\n",
        "fourth_input = data_dir + \"fourth_5000_StudentGenie.csv\"\n",
        "fifth_input = data_dir + \"fifth_5000_StudentGenie.csv\"\n",
        "sixth_input = data_dir + \"sixth_5000_StudentGenie.csv\"\n",
        "seventh_input = data_dir + \"seventh_5000_StudentGenie.csv\"\n",
        "\n",
        "first_output = data_dir + \"norm_first_5000_StudentGenie.csv\"\n",
        "second_output = data_dir + \"norm_second_5000_StudentGenie.csv\"\n",
        "third_output = data_dir + \"norm_third_5000_StudentGenie.csv\"\n",
        "fourth_output = data_dir + \"norm_fourth_5000_StudentGenie.csv\"\n",
        "fifth_output = data_dir + \"norm_fifth_5000_StudentGenie.csv\"\n",
        "sixth_output = data_dir + \"norm_sixth_5000_StudentGenie.csv\"\n",
        "seventh_output = data_dir + \"norm_seventh_5000_StudentGenie.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4hR_TdPLQjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pip install ekphrasis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfdOWsd4QDBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0DqQztFKt2S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "\n",
        "# import argparse\n",
        "import csv\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--train', action='store_true')\n",
        "# parser.add_argument('--dev', action='store_true')\n",
        "# parser.add_argument('--test', action='store_true')\n",
        "\n",
        "\n",
        "def import_data(filename):\n",
        "    data_dict = {}\n",
        "    with open(filename, newline='', encoding='latin') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            data_dict[row['stud']] = row['content']\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def split_sentences(data_dict):\n",
        "    split_dict = {}\n",
        "    for student in data_dict:\n",
        "        sents = data_dict[student].split('|')\n",
        "        sents = [s.strip() for s in sents]\n",
        "        split_dict[student] = sents\n",
        "\n",
        "    return split_dict\n",
        "\n",
        "def old_split_data_dict(split_dict, n):\n",
        "    dicts = [dict() for i in range(n)]\n",
        "    size = len(split_dict)\n",
        "    batch_size = int(size / n)\n",
        "    start = 0\n",
        "    end = 1\n",
        "    for i, student in enumerate(split_dict):\n",
        "        lower = batch_size * start\n",
        "        upper = batch_size * end\n",
        "        if (i >= lower and i < upper) and (start < n):\n",
        "            dicts[start][student] = split_dict[student]\n",
        "            if i + 1 == upper:\n",
        "                start += 1\n",
        "                end += 1\n",
        "\n",
        "    return dicts\n",
        "  \n",
        "def split_data_dict(split_dict, n):\n",
        "    dicts = [dict() for i in range(n+1)]\n",
        "    size = len(split_dict)\n",
        "    batch_size = int(size / n)\n",
        "    start = 0\n",
        "    end = 1\n",
        "    for i, student in enumerate(split_dict):\n",
        "        lower = batch_size * start\n",
        "        upper = batch_size * end\n",
        "        if (i >= lower and i < upper):\n",
        "            dicts[start][student] = split_dict[student]\n",
        "            if i + 1 == upper and start < n:\n",
        "                start += 1\n",
        "                end += 1\n",
        "\n",
        "    return dicts\n",
        "\n",
        "def export_normalized_data(datadict, outfile, normalized):\n",
        "    print(\"Exporting data...\")\n",
        "    with open(outfile, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['stud', 'messages']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for student in normalized:\n",
        "            row = {\n",
        "                'stud': student,\n",
        "                'messages': normalized[student],\n",
        "            }\n",
        "            writer.writerow(row)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzQoZW-_LAYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import itertools\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer, Tokenizer\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "class Normalizer(TextPreProcessor):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.remove_tags = kwargs.get(\"remove_tags\", True)\n",
        "\n",
        "        self.tags = ['<repeated>', '<emphasis>', '<email>']\n",
        "\n",
        "        # add custom stopwords from your dataset here\n",
        "        custom_stopwords = [\n",
        "            'from', \n",
        "            'subject', \n",
        "            're', \n",
        "            'edu', \n",
        "            'use',  \n",
        "            'propername', \n",
        "            'hey', \n",
        "            'dear', \n",
        "            'genie', \n",
        "            'message', \n",
        "            'messages',\n",
        "            'ginie',\n",
        "        ]\n",
        "        custom_stopwords = set(custom_stopwords)\n",
        "        nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "        self.stopwords = custom_stopwords.union(nltk_stopwords)\n",
        "\n",
        "        self.seg = Segmenter()\n",
        "        self.spell = SpellCorrector()\n",
        "\n",
        "        self.elongated = self.regexes[\"elongated\"]\n",
        "        self.mini_elongated = re.compile(\"([a-zA-Z])\\\\1\\\\1\")\n",
        "        self.propername_regex = \"(propername)+\"\n",
        "        self.repeated_digits = re.compile(\"\\d{5,}\")\n",
        "\n",
        "    def handle_repeated_digits(self, sentence: str):\n",
        "        s = re.sub(self.repeated_digits, '', sentence)\n",
        "        return s\n",
        "\n",
        "    def handle_elongated(self, sentence: str):\n",
        "        s = re.sub(self.mini_elongated, '', sentence)\n",
        "        s = self.elongated.sub(\n",
        "            lambda w: self.handle_elongated_match(w), s)\n",
        "        return s\n",
        "\n",
        "    def handle_repeated_propername(self, sentence: str):\n",
        "        regex = re.compile(self.propername_regex)\n",
        "        s = re.sub(regex, '', sentence).strip()\n",
        "        return s\n",
        "\n",
        "    def correct(self, sentence: str):\n",
        "        sentence = sentence.split()\n",
        "        corrected = [self.spell.correct(w) if len(w) < 25 else '' for w in sentence]\n",
        "        return \" \".join(corrected)\n",
        "\n",
        "    def segment(self, sentence: str):\n",
        "        s = sentence.split()\n",
        "        segmented = []\n",
        "        for tok in s:\n",
        "            if len(tok) < 50:\n",
        "                seg_list = self.seg.segment(tok).split()\n",
        "                segmented.extend(seg_list)\n",
        "            else:\n",
        "                segmented.append('')\n",
        "        segmented = list(itertools.chain(segmented))\n",
        "        return \" \".join(segmented)\n",
        "\n",
        "    def strip_tags(self, sentence: list):\n",
        "        if self.remove_tags:\n",
        "            s = sentence\n",
        "            # s = sentence.split()\n",
        "            no_tags = [w for w in s if w not in self.tags]\n",
        "            return \" \".join(no_tags)\n",
        "        else:\n",
        "            return sentence\n",
        "\n",
        "    def remove_stopwords(self, sentence: str):\n",
        "        s = sentence.split()\n",
        "        cleaned = [w for w in s if w.lower() not in self.stopwords]\n",
        "        return \" \".join(cleaned)\n",
        "\n",
        "    def strip_non_alphanumeric(self, sentence: str):\n",
        "        return \"\".join([c for c in sentence if c in string.printable])\n",
        "\n",
        "    def strip_punctuation(self, sentence: str):\n",
        "        '''\n",
        "        Remove all punctuation from the given sentence,\n",
        "        including emojis like :)\n",
        "        '''\n",
        "        punct = string.punctuation\n",
        "        s = sentence.translate(str.maketrans(punct, ' ' * len(punct)))\n",
        "        return \" \".join(s.split())\n",
        "      \n",
        "    def handle_long_strings(self, sentence: str):\n",
        "        if ' ' not in sentence:\n",
        "            p = [c for c in sentence if c in string.punctuation]\n",
        "            if len(sentence) > 50 and not p:\n",
        "                return ''\n",
        "        return sentence\n",
        "\n",
        "    def normalize(self, split_dict):\n",
        "        l = len(split_dict)\n",
        "        normalized = {}\n",
        "        for j, student in enumerate(split_dict):\n",
        "#             print(f\"{j}/{l}\")\n",
        "            if j % 20 == 0:\n",
        "              print(f\"{j}/{l}\")\n",
        "            cleaned = []\n",
        "            for i, msg in enumerate(split_dict[student]):\n",
        "#                 print(msg)\n",
        "                m = msg.lower()\n",
        "                m = self.handle_long_strings(m)\n",
        "                m = self.strip_non_alphanumeric(m)\n",
        "                m = self.pre_process_doc(m)\n",
        "                m = self.strip_tags(m)\n",
        "                m = self.remove_stopwords(m)\n",
        "                m = self.strip_punctuation(m)\n",
        "                m = self.handle_repeated_digits(m)\n",
        "                m = self.handle_elongated(m)\n",
        "                m = self.handle_repeated_propername(m)\n",
        "                m = self.segment(m)\n",
        "                m = self.correct(m)\n",
        "                m = self.remove_stopwords(m)\n",
        "                cleaned.append(m)\n",
        "            cleaned = \" | \".join(cleaned)\n",
        "            normalized[student] = cleaned\n",
        "        return normalized\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnfM7e_kLXwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "# from multiprocessing import Pool\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer, Tokenizer\n",
        "# from normalizer import Normalizer\n",
        "# from utils import import_data, split_sentences, export_normalized_data, split_data_dict\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--train', action='store_true')\n",
        "parser.add_argument('--dev', action='store_true')\n",
        "parser.add_argument('--test', action='store_true')\n",
        "\n",
        "\n",
        "def get_normalizer():\n",
        "    normalizer = Normalizer(\n",
        "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "        #\"elongated\",\n",
        "        annotate={\"repeated\", 'emphasis', 'censored'},\n",
        "        remove_tags=True,\n",
        "        unpack_hashtags=False,\n",
        "        segmenter='english',\n",
        "        spell_correction=False,\n",
        "        spell_correct_elong=False,\n",
        "        tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
        "    )\n",
        "\n",
        "    return normalizer\n",
        "\n",
        "def main(filename, outfile):\n",
        "    data_dict = import_data(filename)\n",
        "    split_dict = split_sentences(data_dict)\n",
        "    \n",
        "    normalizer = get_normalizer()\n",
        "        \n",
        "    norm = normalizer.normalize(split_dict)\n",
        "\n",
        "    export_normalized_data(data_dict, outfile, norm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SckJ_KVLknP",
        "colab_type": "code",
        "outputId": "50dc945b-7d4c-4e08-f3c3-0b6d0f865daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4471
        }
      },
      "cell_type": "code",
      "source": [
        "outfile = sixth_output\n",
        "infile = sixth_input\n",
        "main(infile, outfile)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_2grams.txt\n",
            "Reading english - 1grams ...\n",
            "0/5000\n",
            "20/5000\n",
            "40/5000\n",
            "60/5000\n",
            "80/5000\n",
            "100/5000\n",
            "120/5000\n",
            "140/5000\n",
            "160/5000\n",
            "180/5000\n",
            "200/5000\n",
            "220/5000\n",
            "240/5000\n",
            "260/5000\n",
            "280/5000\n",
            "300/5000\n",
            "320/5000\n",
            "340/5000\n",
            "360/5000\n",
            "380/5000\n",
            "400/5000\n",
            "420/5000\n",
            "440/5000\n",
            "460/5000\n",
            "480/5000\n",
            "500/5000\n",
            "520/5000\n",
            "540/5000\n",
            "560/5000\n",
            "580/5000\n",
            "600/5000\n",
            "620/5000\n",
            "640/5000\n",
            "660/5000\n",
            "680/5000\n",
            "700/5000\n",
            "720/5000\n",
            "740/5000\n",
            "760/5000\n",
            "780/5000\n",
            "800/5000\n",
            "820/5000\n",
            "840/5000\n",
            "860/5000\n",
            "880/5000\n",
            "900/5000\n",
            "920/5000\n",
            "940/5000\n",
            "960/5000\n",
            "980/5000\n",
            "1000/5000\n",
            "1020/5000\n",
            "1040/5000\n",
            "1060/5000\n",
            "1080/5000\n",
            "1100/5000\n",
            "1120/5000\n",
            "1140/5000\n",
            "1160/5000\n",
            "1180/5000\n",
            "1200/5000\n",
            "1220/5000\n",
            "1240/5000\n",
            "1260/5000\n",
            "1280/5000\n",
            "1300/5000\n",
            "1320/5000\n",
            "1340/5000\n",
            "1360/5000\n",
            "1380/5000\n",
            "1400/5000\n",
            "1420/5000\n",
            "1440/5000\n",
            "1460/5000\n",
            "1480/5000\n",
            "1500/5000\n",
            "1520/5000\n",
            "1540/5000\n",
            "1560/5000\n",
            "1580/5000\n",
            "1600/5000\n",
            "1620/5000\n",
            "1640/5000\n",
            "1660/5000\n",
            "1680/5000\n",
            "1700/5000\n",
            "1720/5000\n",
            "1740/5000\n",
            "1760/5000\n",
            "1780/5000\n",
            "1800/5000\n",
            "1820/5000\n",
            "1840/5000\n",
            "1860/5000\n",
            "1880/5000\n",
            "1900/5000\n",
            "1920/5000\n",
            "1940/5000\n",
            "1960/5000\n",
            "1980/5000\n",
            "2000/5000\n",
            "2020/5000\n",
            "2040/5000\n",
            "2060/5000\n",
            "2080/5000\n",
            "2100/5000\n",
            "2120/5000\n",
            "2140/5000\n",
            "2160/5000\n",
            "2180/5000\n",
            "2200/5000\n",
            "2220/5000\n",
            "2240/5000\n",
            "2260/5000\n",
            "2280/5000\n",
            "2300/5000\n",
            "2320/5000\n",
            "2340/5000\n",
            "2360/5000\n",
            "2380/5000\n",
            "2400/5000\n",
            "2420/5000\n",
            "2440/5000\n",
            "2460/5000\n",
            "2480/5000\n",
            "2500/5000\n",
            "2520/5000\n",
            "2540/5000\n",
            "2560/5000\n",
            "2580/5000\n",
            "2600/5000\n",
            "2620/5000\n",
            "2640/5000\n",
            "2660/5000\n",
            "2680/5000\n",
            "2700/5000\n",
            "2720/5000\n",
            "2740/5000\n",
            "2760/5000\n",
            "2780/5000\n",
            "2800/5000\n",
            "2820/5000\n",
            "2840/5000\n",
            "2860/5000\n",
            "2880/5000\n",
            "2900/5000\n",
            "2920/5000\n",
            "2940/5000\n",
            "2960/5000\n",
            "2980/5000\n",
            "3000/5000\n",
            "3020/5000\n",
            "3040/5000\n",
            "3060/5000\n",
            "3080/5000\n",
            "3100/5000\n",
            "3120/5000\n",
            "3140/5000\n",
            "3160/5000\n",
            "3180/5000\n",
            "3200/5000\n",
            "3220/5000\n",
            "3240/5000\n",
            "3260/5000\n",
            "3280/5000\n",
            "3300/5000\n",
            "3320/5000\n",
            "3340/5000\n",
            "3360/5000\n",
            "3380/5000\n",
            "3400/5000\n",
            "3420/5000\n",
            "3440/5000\n",
            "3460/5000\n",
            "3480/5000\n",
            "3500/5000\n",
            "3520/5000\n",
            "3540/5000\n",
            "3560/5000\n",
            "3580/5000\n",
            "3600/5000\n",
            "3620/5000\n",
            "3640/5000\n",
            "3660/5000\n",
            "3680/5000\n",
            "3700/5000\n",
            "3720/5000\n",
            "3740/5000\n",
            "3760/5000\n",
            "3780/5000\n",
            "3800/5000\n",
            "3820/5000\n",
            "3840/5000\n",
            "3860/5000\n",
            "3880/5000\n",
            "3900/5000\n",
            "3920/5000\n",
            "3940/5000\n",
            "3960/5000\n",
            "3980/5000\n",
            "4000/5000\n",
            "4020/5000\n",
            "4040/5000\n",
            "4060/5000\n",
            "4080/5000\n",
            "4100/5000\n",
            "4120/5000\n",
            "4140/5000\n",
            "4160/5000\n",
            "4180/5000\n",
            "4200/5000\n",
            "4220/5000\n",
            "4240/5000\n",
            "4260/5000\n",
            "4280/5000\n",
            "4300/5000\n",
            "4320/5000\n",
            "4340/5000\n",
            "4360/5000\n",
            "4380/5000\n",
            "4400/5000\n",
            "4420/5000\n",
            "4440/5000\n",
            "4460/5000\n",
            "4480/5000\n",
            "4500/5000\n",
            "4520/5000\n",
            "4540/5000\n",
            "4560/5000\n",
            "4580/5000\n",
            "4600/5000\n",
            "4620/5000\n",
            "4640/5000\n",
            "4660/5000\n",
            "4680/5000\n",
            "4700/5000\n",
            "4720/5000\n",
            "4740/5000\n",
            "4760/5000\n",
            "4780/5000\n",
            "4800/5000\n",
            "4820/5000\n",
            "4840/5000\n",
            "4860/5000\n",
            "4880/5000\n",
            "4900/5000\n",
            "4920/5000\n",
            "4940/5000\n",
            "4960/5000\n",
            "4980/5000\n",
            "Exporting data...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}