{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "normalization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vxfpDNOvMLEa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8OAKBS_pMPUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up directory variables\n",
        "data_dir = \"/content/gdrive/My Drive/Term Project 530/Data/\"\n",
        "\n",
        "# student_genie_file = data_dir + \"StudentGenieMessages.csv\"\n",
        "# student_genie_teacher_file = data_dir + \"StudentTeacherMessages.csv\"\n",
        "# short_student_genie_file = data_dir + \"short_StudentGenie.csv\"\n",
        "\n",
        "first_input = data_dir + \"first_5000_StudentGenie.csv\"\n",
        "second_input = data_dir + \"second_5000_StudentGenie.csv\"\n",
        "third_input = data_dir + \"third_5000_StudentGenie.csv\"\n",
        "fourth_input = data_dir + \"fourth_5000_StudentGenie.csv\"\n",
        "fifth_input = data_dir + \"fifth_5000_StudentGenie.csv\"\n",
        "sixth_input = data_dir + \"sixth_5000_StudentGenie.csv\"\n",
        "seventh_input = data_dir + \"seventh_5000_StudentGenie.csv\"\n",
        "\n",
        "first_output = data_dir + \"norm_first_5000_StudentGenie.csv\"\n",
        "second_output = data_dir + \"norm_second_5000_StudentGenie.csv\"\n",
        "third_output = data_dir + \"norm_third_5000_StudentGenie.csv\"\n",
        "fourth_output = data_dir + \"norm_fourth_5000_StudentGenie.csv\"\n",
        "fifth_output = data_dir + \"norm_fifth_5000_StudentGenie.csv\"\n",
        "sixth_output = data_dir + \"norm_sixth_5000_StudentGenie.csv\"\n",
        "seventh_output = data_dir + \"norm_seventh_5000_StudentGenie.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4hR_TdPLQjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pip install ekphrasis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfdOWsd4QDBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z0DqQztFKt2S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "\n",
        "import csv\n",
        "\n",
        "def import_data(filename):\n",
        "    data_dict = {}\n",
        "    with open(filename, newline='', encoding='latin') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            data_dict[row['stud']] = row['content']\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def split_sentences(data_dict):\n",
        "    split_dict = {}\n",
        "    for student in data_dict:\n",
        "        sents = data_dict[student].split('|')\n",
        "        sents = [s.strip() for s in sents]\n",
        "        split_dict[student] = sents\n",
        "\n",
        "    return split_dict\n",
        "\n",
        "def old_split_data_dict(split_dict, n):\n",
        "    dicts = [dict() for i in range(n)]\n",
        "    size = len(split_dict)\n",
        "    batch_size = int(size / n)\n",
        "    start = 0\n",
        "    end = 1\n",
        "    for i, student in enumerate(split_dict):\n",
        "        lower = batch_size * start\n",
        "        upper = batch_size * end\n",
        "        if (i >= lower and i < upper) and (start < n):\n",
        "            dicts[start][student] = split_dict[student]\n",
        "            if i + 1 == upper:\n",
        "                start += 1\n",
        "                end += 1\n",
        "\n",
        "    return dicts\n",
        "  \n",
        "def split_data_dict(split_dict, n):\n",
        "    dicts = [dict() for i in range(n+1)]\n",
        "    size = len(split_dict)\n",
        "    batch_size = int(size / n)\n",
        "    start = 0\n",
        "    end = 1\n",
        "    for i, student in enumerate(split_dict):\n",
        "        lower = batch_size * start\n",
        "        upper = batch_size * end\n",
        "        if (i >= lower and i < upper):\n",
        "            dicts[start][student] = split_dict[student]\n",
        "            if i + 1 == upper and start < n:\n",
        "                start += 1\n",
        "                end += 1\n",
        "\n",
        "    return dicts\n",
        "\n",
        "def export_normalized_data(datadict, outfile, normalized):\n",
        "    print(\"Exporting data...\")\n",
        "    with open(outfile, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['stud', 'messages']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for student in normalized:\n",
        "            row = {\n",
        "                'stud': student,\n",
        "                'messages': normalized[student],\n",
        "            }\n",
        "            writer.writerow(row)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzQoZW-_LAYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import itertools\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer, Tokenizer\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "class Normalizer(TextPreProcessor):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.remove_tags = kwargs.get(\"remove_tags\", True)\n",
        "\n",
        "        self.tags = ['<repeated>', '<emphasis>', '<email>']\n",
        "\n",
        "        # add custom stopwords from your dataset here\n",
        "        custom_stopwords = [\n",
        "            'from', \n",
        "            'subject', \n",
        "            're', \n",
        "            'edu', \n",
        "            'use',  \n",
        "            'propername', \n",
        "            'hey', \n",
        "            'dear', \n",
        "            'genie', \n",
        "            'message', \n",
        "            'messages',\n",
        "            'ginie',\n",
        "        ]\n",
        "        custom_stopwords = set(custom_stopwords)\n",
        "        nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "        self.stopwords = custom_stopwords.union(nltk_stopwords)\n",
        "\n",
        "        self.seg = Segmenter()\n",
        "        self.spell = SpellCorrector()\n",
        "\n",
        "        self.elongated = self.regexes[\"elongated\"]\n",
        "        self.mini_elongated = re.compile(\"([a-zA-Z])\\\\1\\\\1\")\n",
        "        self.propername_regex = \"(propername)+\"\n",
        "        self.repeated_digits = re.compile(\"\\d{5,}\")\n",
        "\n",
        "    def handle_repeated_digits(self, sentence: str):\n",
        "        s = re.sub(self.repeated_digits, '', sentence)\n",
        "        return s\n",
        "\n",
        "    def handle_elongated(self, sentence: str):\n",
        "        s = re.sub(self.mini_elongated, '', sentence)\n",
        "        s = self.elongated.sub(\n",
        "            lambda w: self.handle_elongated_match(w), s)\n",
        "        return s\n",
        "\n",
        "    def handle_repeated_propername(self, sentence: str):\n",
        "        regex = re.compile(self.propername_regex)\n",
        "        s = re.sub(regex, '', sentence).strip()\n",
        "        return s\n",
        "\n",
        "    def correct(self, sentence: str):\n",
        "        sentence = sentence.split()\n",
        "        corrected = [self.spell.correct(w) if len(w) < 25 else '' for w in sentence]\n",
        "        return \" \".join(corrected)\n",
        "\n",
        "    def segment(self, sentence: str):\n",
        "        s = sentence.split()\n",
        "        segmented = []\n",
        "        for tok in s:\n",
        "            if len(tok) < 50:\n",
        "                seg_list = self.seg.segment(tok).split()\n",
        "                segmented.extend(seg_list)\n",
        "            else:\n",
        "                segmented.append('')\n",
        "        segmented = list(itertools.chain(segmented))\n",
        "        return \" \".join(segmented)\n",
        "\n",
        "    def strip_tags(self, sentence: list):\n",
        "        if self.remove_tags:\n",
        "            s = sentence\n",
        "            # s = sentence.split()\n",
        "            no_tags = [w for w in s if w not in self.tags]\n",
        "            return \" \".join(no_tags)\n",
        "        else:\n",
        "            return sentence\n",
        "\n",
        "    def remove_stopwords(self, sentence: str):\n",
        "        s = sentence.split()\n",
        "        cleaned = [w for w in s if w.lower() not in self.stopwords]\n",
        "        return \" \".join(cleaned)\n",
        "\n",
        "    def strip_non_alphanumeric(self, sentence: str):\n",
        "        return \"\".join([c for c in sentence if c in string.printable])\n",
        "\n",
        "    def strip_punctuation(self, sentence: str):\n",
        "        '''\n",
        "        Remove all punctuation from the given sentence,\n",
        "        including emojis like :)\n",
        "        '''\n",
        "        punct = string.punctuation\n",
        "        s = sentence.translate(str.maketrans(punct, ' ' * len(punct)))\n",
        "        return \" \".join(s.split())\n",
        "      \n",
        "    def handle_long_strings(self, sentence: str):\n",
        "        if ' ' not in sentence:\n",
        "            p = [c for c in sentence if c in string.punctuation]\n",
        "            if len(sentence) > 50 and not p:\n",
        "                return ''\n",
        "        return sentence\n",
        "\n",
        "    def normalize(self, split_dict):\n",
        "        l = len(split_dict)\n",
        "        normalized = {}\n",
        "        for j, student in enumerate(split_dict):\n",
        "            if j % 20 == 0:\n",
        "              print(f\"{j}/{l}\")\n",
        "            cleaned = []\n",
        "            for i, msg in enumerate(split_dict[student]):\n",
        "                m = msg.lower()\n",
        "                m = self.handle_long_strings(m)\n",
        "                m = self.strip_non_alphanumeric(m)\n",
        "                m = self.pre_process_doc(m)\n",
        "                m = self.strip_tags(m)\n",
        "                m = self.remove_stopwords(m)\n",
        "                m = self.strip_punctuation(m)\n",
        "                m = self.handle_repeated_digits(m)\n",
        "                m = self.handle_elongated(m)\n",
        "                m = self.handle_repeated_propername(m)\n",
        "                m = self.segment(m)\n",
        "                m = self.correct(m)\n",
        "                m = self.remove_stopwords(m)\n",
        "                cleaned.append(m)\n",
        "            cleaned = \" | \".join(cleaned)\n",
        "            normalized[student] = cleaned\n",
        "        return normalized\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnfM7e_kLXwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer, Tokenizer\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--train', action='store_true')\n",
        "parser.add_argument('--dev', action='store_true')\n",
        "parser.add_argument('--test', action='store_true')\n",
        "\n",
        "\n",
        "def get_normalizer():\n",
        "    normalizer = Normalizer(\n",
        "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "        #\"elongated\",\n",
        "        annotate={\"repeated\", 'emphasis', 'censored'},\n",
        "        remove_tags=True,\n",
        "        unpack_hashtags=False,\n",
        "        segmenter='english',\n",
        "        spell_correction=False,\n",
        "        spell_correct_elong=False,\n",
        "        tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
        "    )\n",
        "\n",
        "    return normalizer\n",
        "\n",
        "def main(filename, outfile):\n",
        "    data_dict = import_data(filename)\n",
        "    split_dict = split_sentences(data_dict)\n",
        "    \n",
        "    normalizer = get_normalizer()\n",
        "        \n",
        "    norm = normalizer.normalize(split_dict)\n",
        "\n",
        "    export_normalized_data(data_dict, outfile, norm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SckJ_KVLknP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "outfile = sixth_output\n",
        "infile = sixth_input\n",
        "main(infile, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}